from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
import os
from rag_tools import rag_tools
from langgraph.graph import MessagesState

RAG_GROUNDING_MODEL=os.environ.get("RAG_GROUNDING_MODEL")

rag_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful AI assistant that answers Insurance agents' questions on Real-Time clause and Benefit lookup. "
            "You have access to a tool that retrieves relevant context from Evidence of coverage and Summary of Benefits documents "
            "Use the tool to find relevant information before answering questions. "
            "Always cite the source and page_num you use in your answers. "
            "If you cannot find the answer in the retrieved documentation, say so."
        ),
        MessagesPlaceholder(variable_name="rag_messages"),
    ]
)

rag_llm = ChatOpenAI(model=os.environ.get("GPT_MODEL"), temperature=0, api_key=os.environ.get("OPENAI_API_KEY")).bind_tools(rag_tools)
rag_chain = rag_prompt | rag_llm


rag_grounding_prompt = ChatPromptTemplate.from_messages(
    [
        ("""
    You are a verification model responsible for preventing hallucinations in a RAG system.
    
    You will be given:
    1. A user question
    2. Retrieved context passages
    3. A candidate answer generated by another model
    
    Your task:
    - Determine whether the candidate answer is fully supported by the provided context.
    - If the answer contains any information that is not explicitly supported by the context, mark it as NOT GROUNDED.
    - If the answer is grounded, return it unchanged.
    - If the answer is not grounded, rewrite a corrected answer using ONLY the information found in the context. If the context does not contain enough information, respond with: "The context does not provide enough information to answer this question."
    
    Rules:
    - Do NOT add new facts.
    - Do NOT guess or infer beyond what is explicitly stated.
    - Do NOT rely on prior knowledge.
    - Only use the provided context.
    
    Here is the information:
    
    [QUESTION] 
    {question} 
    
    [CONTEXT] 
    {context} 
    
    [ANSWER] 
    {answer}
    
    Output format (JSON):
    {{
      "is_grounded": true/false,
      "final_answer": "your grounded answer here"
    }}
    """),
        # MessagesPlaceholder(variable_name="rag_messages"),
    ]
)
rag_llm = ChatOpenAI(model=os.environ.get("RAG_GROUNDING_MODEL"), temperature=0, api_key=os.environ.get("OPENAI_API_KEY"))
rag_grounding_chain = (
    {
        "question": lambda x: x["question"],
        "context": lambda x: x["context"],
        "answer": lambda x: x["answer"]
    }
    | rag_grounding_prompt
    | rag_llm
)
